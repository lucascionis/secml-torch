{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f46692f",
   "metadata": {},
   "source": [
    "# Train DNNs with SecML-Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d370ca2",
   "metadata": {},
   "source": [
    "In this notebook, we will use the basic training functionalities of SecML-Torch to train a regular PyTorch Deep Neural Network (DNN) classifier.\n",
    "\n",
    "We will train a classifier for the MNIST dataset.\n",
    "First, we define the model as a `torch.nn.Module`, as usually done in the `torch` library. The model is a simple fully-connected network with three layers. Notice that this is standard PyTorch code - SecML-Torch is designed to work with existing PyTorch models without modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9302b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "try:\n",
    "    import secmlt\n",
    "except ImportError:\n",
    "   %pip install git+https://github.com/pralab/secml-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9dc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MNISTNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(784, 200)\n",
    "        self.fc2 = torch.nn.Linear(200, 200)\n",
    "        self.fc3 = torch.nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "net = MNISTNet()\n",
    "device = \"cpu\"\n",
    "net = net.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f048b",
   "metadata": {},
   "source": [
    "We import the training and testing dataset of MNIST from `torchvision`, and provide them to the dedicated data loaders. We use a batch size of 64 and set `shuffle=False` to ensure reproducible results across runs. The `ToTensor()` transform automatically converts PIL images to tensors and scales pixel values to the [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a48a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torchvision.datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_path = \"data/datasets/\"\n",
    "training_dataset = torchvision.datasets.MNIST(\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    train=True,\n",
    "    root=dataset_path,\n",
    "    download=True,\n",
    ")\n",
    "training_data_loader = DataLoader(training_dataset, batch_size=64, shuffle=False)\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    train=False,\n",
    "    root=dataset_path,\n",
    "    download=True,\n",
    ")\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c2ae32",
   "metadata": {},
   "source": [
    "Finally, we initialize the optimizer to use for training the model. We're using Adam with a learning rate of 1e-3, which is a good default choice for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710ac94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "optimizer = Adam(lr=1e-3, params=net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221642e6",
   "metadata": {},
   "source": [
    "Now we will start using the SecML-Torch functionalities to train the previously-defined model on the MNIST dataset just loaded.\n",
    "\n",
    "We will use the class `secmlt.models.pytorch.base_pytorch_trainer.BasePyTorchTrainer` to prepare a training loop. \n",
    "This class implements the regular training loop which performs optimization steps (with the optimizer of choice) on a for loop on the batches of samples, for a given amount of epochs (passed as an input parameter). The trainer handles the forward pass, loss computation, backward pass, and parameter updates automatically.\n",
    "\n",
    "We wrap the model into a `secmlt.models.pytorch.base_pytorch_nn.BasePytorchClassifier` class, which provides the APIs to use models subclassing the `torch.nn.Module` within SecML-Torch. This wrapper doesn't modify your model but adds methods that integrate with SecML-Torch's ecosystem for attacks and defenses.\n",
    "Then, we can train our model by calling `model.train(dataloader=training_data_loader)`. This single line replaces the typical PyTorch training loop boilerplate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c810215a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTNet(\n",
       "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from secmlt.models.pytorch.base_pytorch_nn import BasePytorchClassifier\n",
    "from secmlt.models.pytorch.base_pytorch_trainer import BasePyTorchTrainer\n",
    "\n",
    "# Training MNIST model\n",
    "trainer = BasePyTorchTrainer(optimizer=optimizer, epochs=1)\n",
    "model = BasePytorchClassifier(model=net, trainer=trainer)\n",
    "model.train(dataloader=training_data_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930056dd",
   "metadata": {},
   "source": [
    "We can check how the model performs on the testing dataset by using the `secmlt.metrics.classification.Accuracy` wrapper. \n",
    "This provides the accuracy scoring loop that queries the model with all the batches and counts how many predictions are correct. The metric automatically handles device placement and batch aggregation, returning a single accuracy value for the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da47818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  tensor(0.9498)\n"
     ]
    }
   ],
   "source": [
    "from secmlt.metrics.classification import Accuracy\n",
    "\n",
    "# Test MNIST model\n",
    "accuracy = Accuracy()(model, test_data_loader)\n",
    "print(\"test accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b037e06",
   "metadata": {},
   "source": [
    "Finally, we can save our model weights with the `torch` saving functionalities.\n",
    "To get the model, we can access the `model` attribute of the `secmlt.models.pytorch.base_pytorch_nn.BasePytorchClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60dc8898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = Path(\"data/models/mnist\")\n",
    "if not model_path.exists():\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.model.state_dict(), model_path / \"mnist_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f345a",
   "metadata": {},
   "source": [
    "After saving the `model` we can load it for further evaluations by simply using the `torch.load` method and wrap it again with `secmlt.models.pytorch.base_pytorch_nn.BasePytorchClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9610fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_net = MNISTNet()\n",
    "model_weights_path = model_path / \"mnist_model.pt\"\n",
    "model_weights = torch.load(model_weights_path, map_location=\"cpu\")\n",
    "trained_net.eval()\n",
    "trained_net.load_state_dict(model_weights)\n",
    "trained_model = BasePytorchClassifier(model=trained_net, trainer=trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secmlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
